{
  "id": "ollama",
  "name": "Ollama",
  "description": "Run large language models locally - Llama 3, Mistral, and more",
  "version": "latest",
  "icon": "https://cdn.simpleicons.org/ollama/000000",
  "category": "AI",
  "docker_compose": {
    "version": "3.8",
    "services": {
      "ollama": {
        "image": "ollama/ollama:latest",
        "ports": ["11434:11434"],
        "volumes": ["ollama_data:/root/.ollama"],
        "restart": "always"
      }
    },
    "volumes": {
      "ollama_data": {}
    }
  },
  "ports": [11434],
  "volumes": ["ollama_data"],
  "environment": {},
  "min_memory": 4096,
  "min_cpu": 4,
  "tags": ["ai", "llm", "machine-learning", "local"],
  "author": "Ollama",
  "website": "https://ollama.ai"
}
